# Linear Algebra

- Vectors - has magnitude and direction or an abstract direction.

- Unit vector - vector with magnitude 1.
  
  ![](https://i.ytimg.com/vi/fMa7bu7gKNo/maxresdefault.jpg)

- Dot product
  
   ![](https://andymath.com/wp-content/uploads/2019/07/dot-product-2.jpg)
  
- Matrix - a rectangular array of numbers.

- Eigen vectors
  - Vectors that remain in the same direction after a linear transformation.
  - Do not change direction under transformation.

- Eigen values
  - Scalars associated with eigen vectors.
  - Represent the factor by which the eigen vector is scaled during the transformation.

- Projection of a matrix
  
  ![](https://th.bing.com/th/id/OIP.zFxXa3BscVhg4ndLUF4IKQAAAA?rs=1&pid=ImgDetMain)

- Norm of a vector is magnitude.

- UV Decomposition
  - UV Decomposition,more commonly known as Singular Value Decomposition(SVD).
  - Matrix factorization technique that decomposes a matrix into three other matrices.
  - A powerful tool in linear algebra with numerous applications in data science and machine learning.

- Principal Component Analysis(PCA)
  -  PCA is a statistical procedure that uses an orthogonal transformation that converts a set of correlated variables to a set of uncorrelated variables.PCA is the most widely used tool in exploratory data analysis and in machine learning for predictive models.
  -  Principal Component Analysis (PCA) is an unsupervised learning algorithm technique used to examine the interrelations among a set of variables. It is also known as a general factor analysis where regression determines a line of best fit.
  -  The main goal of Principal Component Analysis (PCA) is to reduce the dimensionality of a dataset while preserving the most important patterns or relationships between the variables without any prior knowledge of the target variables.  
      

 
